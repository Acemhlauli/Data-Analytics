# -*- coding: utf-8 -*-
"""Mhlauli_A-Assignment_Three_ Demographic_Profiling_of_Purchase_Intent_Using_Probabilistic_Models .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tJzrAkETmzYWn67-_1Gy6bS-oaozigqZ

#**Introduction**

This assignment builds directly on the work from Assignment 2. The primary objective is to use the Bayesian networks (both the original and the synthetically balanced versions) to identify which specific combinations of demographic variables are most predictive of the four different purchase intention types. By calculating conditional probabilities, this analysis will reveal the most likely customer profiles for each intention category and explore how data balancing techniques impact these demographic insights.

The goal of this assignment is to use the Bayesian network models we built in Assignment 2 to determine which combinations of demographic variables are most likely to lead to each of the four purchase intention types: `I intend to purchase from this grocery store`, `I would like to repeat my experience in this kind of grocery store`, `I would purchase from this grocery store in the future`, and `I would recommend purchasing in this grocery store to others`. We will perform this analysis on networks trained on both the original (unbalanced) data and the synthetically balanced data to critically evaluate the impact of data balancing on probabilistic inference.

**Objective:**

To determine which combinations of demographic variables are most likely to result in each of the four purchase intention types by analysing both balanced and unbalanced Bayesian networks developed in Assignment 2.



---


*    [Assignment 2](https://colab.research.google.com/drive/1C_mFK70YVFOUmjP2BuAJivnE0k2XhY54?usp=sharing)
*   [Paper: Data modelling of subsistence retail consumer purchase behavior in South Africa by Author Zulu and Nkuna](https://www.sciencedirect.com/science/article/pii/S2352340922003043)
- [Data: Subsistence Retail Consumer Data](https://data.mendeley.com/datasets/5z37z85jck/1)

## **Dependencies**

###**Installing Libraries**
"""

# Install pgmpy for Bayesian network analysis and other essential libraries
!pip install pandas numpy pickle-mixin pgmpy matplotlib seaborn bnlearn networkx openpyxl

"""###**Importing dependencies**"""

# Import necessary libraries
import pickle
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pgmpy.inference import VariableElimination
import bnlearn as bn
import itertools
# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 100

"""#**Processing**

#**Data and Model Preparation**

## **Part 1: Loading Data and Pre-trained Models**

In this section, we load the essential components from Assignment 2:
1.  The Bayesian network model trained on the **original, unbalanced data**.
2.  The Bayesian network model trained on the **SMOTE-balanced data**.
3.  *(Add any other models you created, e.g., GAN-balanced model)*
4.  The original DataFrame, which helps us identify the demographic variables and their possible states.
"""

from google.colab import files
uploaded = files.upload()

# Load the dictionary of models from the single file
try:
    with open('/content/Original_bayesian_models.pkl', 'rb') as f:
        original_models = pickle.load(f)
        print("Original models loaded successfully.")
    with open('/content/SMOTE_bayesian_models.pkl', 'rb') as f:
        smote_models = pickle.load(f)
    print("SMOTE models loaded successfully.")

    with open('/content/ML_bayesian_models.pkl', 'rb') as f:
        ml_models = pickle.load(f)
    print("ML models loaded successfully.")

    with open('/content/GAN_bayesian_models.pkl', 'rb') as f:
        gan_models = pickle.load(f)
    print("GAN models loaded successfully.")

    # Combine all models into a single dictionary for easier iteration later
    models = {**original_models,**smote_models, **ml_models, **gan_models}
    print("\nDictionary of all models created successfully.")
    print("Available models:", list(models.keys()))


except FileNotFoundError as e:
    print(f"Error loading model file: {e}. Please ensure the correct files are uploaded.")
    models = None # Set to None to indicate models were not loaded
except Exception as e:
    print(f"An error occurred while loading the models: {e}")
    models = None

df = pd.read_csv('/content/Subsistence Retail Consumer Data.csv')
display(df.head())

"""**Demographic Variables**:

- Extract and analyse demographic attributes.
- Prepare a matrix of all plausible combinations of these demographics for all trained networks.
- Define Demographic Variables with Clear Mapping.
- Identify Available Demographics Per Model.
"""

# Define demographic variables and their corresponding mappings
demographic_vars = {
    'Gender': {1: 'Male', 2: 'Female', 3: 'Prefer not to say'},
    'Age': {1: '18–22', 2: '23–28', 3: '29–35', 4: '35–49', 5: '50–65'},
    'Marital Status': {1: 'Married', 2: 'Single', 3: 'Prefer not to say'},
    'Employment Status': {1: 'Employed', 2: 'Unemployed'},
    'Level of Education': {1: 'No formal education', 2: 'Basic education', 3: 'Diploma', 4: 'Degree', 5: 'Postgraduate degree'},
    'Regular Customer': {1: 'Regular', 2: 'Need-based'},
    'Shopping frequency': {1: '1–2 times per week', 2: '2–3 times per week', 3: '3–4 times per week', 4: '5–6 times per week', 5: '6–7 times per week'}
}

# Create an empty list to store data for the DataFrame
demographic_data = []

# Iterate through each demographic variable and its mapping
for category, mapping in demographic_vars.items():
    # Calculate frequency and percentage for each characteristic in the current category
    value_counts = df[category].value_counts().sort_index()
    percentage = df[category].value_counts(normalize=True).sort_index() * 100

    # Add rows to the list
    for characteristic_code, frequency in value_counts.items():
        characteristic_label = mapping.get(characteristic_code, characteristic_code) # Use mapping if available
        percentage_value = percentage.get(characteristic_code, 0).round(1)
        demographic_data.append([category, characteristic_label, frequency, percentage_value])

# Create the pandas DataFrame
demographic_table_df = pd.DataFrame(demographic_data, columns=['Category', 'Characteristic', 'Frequency', 'Percentage (%)'])

# Display the DataFrame
display(demographic_table_df)

# -----------------------------------------------------------------------------
# Define Demographic Variables with Clear Mapping
# -----------------------------------------------------------------------------
print("\nDemographic Variables Definition")
print(f"\n✓ Defined {len(demographic_vars)} demographic variables:")
for var in demographic_vars.keys():
    print(f"  - {var}: {len(demographic_vars[var])} categories")

# -----------------------------------------------------------------------------
# Identify Available Demographics Per Model
# -----------------------------------------------------------------------------
print("\nModel-Specific Demographic Analysis")

def analyze_model_demographics(models_dict, model_type_name):
    """
    Analyze which demographics are available in each model.
    CRITICAL: This ensures we only use variables that exist in the model.
    """
    print(f"\n{model_type_name} Models:")
    model_demographics = {}

    for model_name, model_info in models_dict.items():
        model = model_info.get('model')
        if model:
            model_nodes = set(model.nodes())
            available_demographics = [var for var in demographic_vars.keys()
                                     if var in model_nodes]
            model_demographics[f"{model_type_name}_{model_name}"] = {
                'model': model,
                'demographics': available_demographics,
                'all_nodes': model_nodes
            }
            print(f"  {model_name}: {len(available_demographics)} demographics available")
            print(f"    → {', '.join(available_demographics)}")

    return model_demographics

# Analyze all model types
all_models_info = {}
all_models_info.update(analyze_model_demographics(original_models, "Original"))
all_models_info.update(analyze_model_demographics(smote_models, "SMOTE"))
all_models_info.update(analyze_model_demographics(ml_models, "ML"))
all_models_info.update(analyze_model_demographics(gan_models, "GAN"))

print(f"\n✓ Total models analyzed: {len(all_models_info)}")

"""### **Generate ALL PLAUSIBLE Demographic Combinations Per Mode**l"""

# Define the demographic variables and their states based on the loaded data
# We'll use the unique values from the original DataFrame for each demographic column
demographic_vars_and_states = {
    col: df[col].unique().tolist() for col in demographic_vars.keys()
}

# Generate all possible combinations of demographic states
# The result is a list of dictionaries, where each dictionary represents a unique demographic profile
all_demographic_combinations = [
    dict(zip(demographic_vars_and_states.keys(), combination))
    for combination in itertools.product(*demographic_vars_and_states.values())
]

print(f"Generated {len(all_demographic_combinations)} unique demographic combinations.")
# Display the first 10 combinations as an example
print("\nFirst 10 demographic combinations:")
for i, combo in enumerate(all_demographic_combinations[:10]):
    print(combo)

# You can now use this list of combinations for inference on your Bayesian networks.

# -----------------------------------------------------------------------------
#  Generate ALL PLAUSIBLE Demographic Combinations Per Model
# -----------------------------------------------------------------------------
print("\nGenerating Demographic Combinations")
def generate_combinations_per_model(df, model_info_dict, demographic_vars):
    all_combinations = {}
    for model_key, info in model_info_dict.items():
        demo_vars = info['demographics']
        if not demo_vars:
            print(f"⚠ {model_key}: No demographic variables found - skipping")
            continue
        # Get unique values for each demographic in THIS model
        vars_and_states = {}
        for var in demo_vars:
            unique_vals = sorted(df[var].dropna().unique().astype(int).tolist())
            vars_and_states[var] = unique_vals
        # Generate all combinations using Cartesian product
        combinations = []
        for combo in itertools.product(*vars_and_states.values()):
            combinations.append(dict(zip(vars_and_states.keys(), combo)))

        all_combinations[model_key] = {
            'combinations': combinations,
            'demographics': demo_vars,
            'count': len(combinations)
        }

        print(f"\n{model_key}:")
        print(f"  Demographics: {', '.join(demo_vars)}")
        print(f"  Total combinations: {len(combinations):,}")

    # Summary statistics
    total_combos = sum(info['count'] for info in all_combinations.values())
    print(f"\n{'='*60}")
    print(f"SUMMARY: Generated {total_combos:,} total combinations across all models")
    print(f"{'='*60}")

    return all_combinations

model_combinations = generate_combinations_per_model(df, all_models_info, demographic_vars)

"""**EXPLANATION OF DEMOGRAPHIC COMBINATIONS:**

1. **MODEL-SPECIFIC APPROACH**:
   - Each Bayesian network was trained with different variable selections
   - We identified which demographics exist in each specific model
   - Combinations are generated ONLY for available variables
   - This prevents inference errors and ensures valid queries

2. **COMBINATION GENERATION METHOD:**
   - Used Cartesian product (itertools.product) to generate ALL possible combinations
   - Each demographic can take specific values (e.g., Gender: 1, 2, 3)
   - Total combinations = product of all category counts
   - Example: If 3 demographics with [3, 5, 2] values → 3×5×2 = 30 combinations

3. **PLAUSIBILITY:**
   - All combinations are technically plausible from the data
   - Each combination represents a potential customer profile
   - Real-world feasibility is evaluated during interpretation phase
   - Rare combinations may have similar probabilities due to sparse data

4. **WHY THIS MATTERS:**
   - Comprehensive analysis requires testing ALL demographic scenarios
   - Identifies edge cases and unexpected high-probability profiles
   - Enables complete comparison between balanced and unbalanced models
   - Provides exhaustive input for marketing segmentation strategies

## **Part 2: Probabilistic Inference and Analysis**

Now for the core of the assignment. We will systematically query our models to find the probability of each purchase intention (`I intend to purchase from this grocery store`, `I would like to repeat my experience in this kind of grocery store`, `I would purchase from this grocery store in the future`, and `I would recommend purchasing in this grocery store to others`) given different demographic profiles.

To do this, we'll first identify our **demographic variables** (our evidence) and our **target variable** (`Intention`). We will then perform inference for every possible combination of demographic states.

---

###**Calculate Conditional Probabilities**
"""

# -----------------------------------------------------------------------------
# Purchase Intention State Mapping
# -----------------------------------------------------------------------------
print("\nPurchase Intention Categories")

# Define intention states and their meanings
intention_states = {
    'PI1': 'I intend to purchase from this grocery store',
    'PI2': 'I would like to repeat my experience in this kind of grocery store',
    'PI3': 'I would purchase from this grocery store in the future',
    'PI4': 'I would recommend purchasing in this grocery store to others'
}

intention_values = {
    1: 'Strongly Disagree',
    2: 'Disagree',
    3: 'Neutral',
    4: 'Agree',
    5: 'Strongly Agree'
}

print("\nPurchase Intention Types:")
for pi, description in intention_states.items():
    print(f"  {pi}: {description}")

print("\nLikert Scale Values:")
for val, label in intention_values.items():
    print(f"  {val}: {label}")

# Get actual states from data
intention_states_map = {}
for i in range(1, 5):
    col = f'PI{i}'
    intention_states_map[col] = sorted(df[col].dropna().unique().astype(int).tolist())
    print(f"\n{col} actual states in data: {intention_states_map[col]}")

# -----------------------------------------------------------------------------
# Bayesian Inference Engine
# -----------------------------------------------------------------------------
print("\nBayesian Inference Implementation")

def perform_bayesian_inference(model, target_var, evidence_dict, intention_states_map):
    try:
        # Initialize inference engine
        inference = VariableElimination(model)

        # Perform query: P(target_var | evidence)
        result = inference.query(variables=[target_var], evidence=evidence_dict)

        # Extract probabilities
        states = intention_states_map[target_var]
        probabilities = {state: float(result.values[i]) for i, state in enumerate(states)}

        return probabilities

    except Exception as e:
        print(f"Inference error for {target_var} with evidence {evidence_dict}: {str(e)}")
        return None

# Test the inference engine
print("\nTesting Inference Engine")
test_model_key = list(all_models_info.keys())[0]
test_model = all_models_info[test_model_key]['model']
test_target = test_model_key.split('_')[-1]
test_evidence = model_combinations[test_model_key]['combinations'][0]

print(f"Test Model: {test_model_key}")
print(f"Test Target: {test_target}")
print(f"Test Evidence: {test_evidence}")

test_result = perform_bayesian_inference(test_model, test_target, test_evidence, intention_states_map)
print(f"Test Result: {test_result}")
print("✓ Inference engine working correctly")

# -----------------------------------------------------------------------------
# Comprehensive Inference Across All Models and Combinations
# -----------------------------------------------------------------------------
print("Running Comprehensive Inference")
print("\nThis will calculate probabilities for ALL combinations across ALL models.")
print("Expected computation: Several thousand inferences\n")

def run_comprehensive_inference(all_models_info, model_combinations, intention_states_map):
    all_results = []
    total_inferences = sum(combo['count'] for combo in model_combinations.values())
    completed = 0

    print(f"Starting inference for {total_inferences:,} total queries...\n")

    for model_key, model_info in all_models_info.items():
        model = model_info['model']
        target_var = model_key.split('_')[-1]  # Extract PI1, PI2, etc.

        if model_key not in model_combinations:
            print(f"⚠ Skipping {model_key}: No combinations generated")
            continue

        combinations = model_combinations[model_key]['combinations']
        model_type = '_'.join(model_key.split('_')[:-1])  # SMOTE, ML, or GAN

        print(f"Processing {model_key} ({len(combinations):,} combinations)...")

        for idx, evidence in enumerate(combinations):
            # Progress indicator
            completed += 1
            if completed % 1000 == 0 or completed == total_inferences:
                progress = (completed / total_inferences) * 100
                print(f"  Progress: {completed:,}/{total_inferences:,} ({progress:.1f}%)")

            # Perform inference
            probs = perform_bayesian_inference(model, target_var, evidence, intention_states_map)

            if probs is None:
                continue

            # Store result
            result_entry = {
                'Model_Type': model_type,
                'Target': target_var,
                'Model_Full': model_key,
                **{f'{k}_Code': v for k, v in evidence.items()},
                **{f'P_{target_var}_State_{state}': prob for state, prob in probs.items()},
                'Max_Probability': max(probs.values()),
                'Max_Probability_State': max(probs, key=probs.get),
                'Entropy': -sum(p * np.log(p) if p > 0 else 0 for p in probs.values())
            }

            all_results.append(result_entry)

    print(f"\n✓ Inference complete: {len(all_results):,} valid results")
    return pd.DataFrame(all_results)
# Run the comprehensive inference
results_df = run_comprehensive_inference(all_models_info, model_combinations, intention_states_map)

# Display results
print("\nInference Results Summary")
print(f"Total results: {len(results_df):,}")
print(f"\nColumns: {list(results_df.columns)}")
print(f"\nFirst 5 results:")
display(results_df.head())

print("\nStatistical Summary")
prob_cols = [col for col in results_df.columns if col.startswith('P_')]
display(results_df[prob_cols].describe())

# -----------------------------------------------------------------------------
#  Ranking Top Combinations Per Category
# -----------------------------------------------------------------------------
print("\nRanking Top Demographic Combinations")

def rank_top_combinations(results_df, demographic_vars, top_n=20):
    rankings = []

    for model_type in results_df['Model_Type'].unique():
        for target in results_df['Target'].unique():
            model_df = results_df[
                (results_df['Model_Type'] == model_type) &
                (results_df['Target'] == target)
            ]

            if len(model_df) == 0:
                continue

            # Get probability columns for this target
            prob_cols = [col for col in model_df.columns if col.startswith(f'P_{target}_')]

            for prob_col in prob_cols:
                state = int(prob_col.split('_State_')[1])
                state_label = intention_values.get(state, str(state))

                # Get top N combinations for this state
                top_combos = model_df.nlargest(top_n, prob_col).copy()

                for rank, (idx, row) in enumerate(top_combos.iterrows(), 1):
                    ranking_entry = {
                        'Model_Type': model_type,
                        'Target': target,
                        'Intention_Meaning': intention_states.get(target, target),
                        'State_Value': state,
                        'State_Label': state_label,
                        'Rank': rank,
                        'Probability': row[prob_col]
                    }

                    # Add demographic codes and labels
                    for demo_var in demographic_vars.keys():
                        code_col = f'{demo_var}_Code'
                        if code_col in row.index and pd.notna(row[code_col]):
                            ranking_entry[demo_var] = row[code_col]
                            ranking_entry[f'{demo_var}_Label'] = demographic_vars[demo_var].get(
                                int(row[code_col]), 'Unknown'
                            )
                        elif code_col in row.index:
                             ranking_entry[demo_var] = None # Or a placeholder like 'N/A'
                             ranking_entry[f'{demo_var}_Label'] = 'Not Applicable'


                    rankings.append(ranking_entry)

    return pd.DataFrame(rankings)

rankings_df = rank_top_combinations(results_df, demographic_vars, top_n=20)

print(f"\n✓ Generated rankings: {len(rankings_df):,} entries")
print(f"\nTop 10 overall:")
display(rankings_df.head(10))

def rank_top_combinations(results_df, demographic_vars, all_models_info, top_n=20):
    rankings = []

    for model_type in results_df['Model_Type'].unique():
        for target in results_df['Target'].unique():
            model_df = results_df[
                (results_df['Model_Type'] == model_type) &
                (results_df['Target'] == target)
            ]

            if len(model_df) == 0:
                continue

            # Get the model key to find which demographics it uses
            model_key = f"{model_type}_{target}"
            available_demos = all_models_info[model_key]['demographics']

            # Get probability columns for this target
            prob_cols = [col for col in model_df.columns if col.startswith(f'P_{target}_')]

            for prob_col in prob_cols:
                state = int(prob_col.split('_State_')[1])
                state_label = intention_values.get(state, str(state))

                # Get top N combinations for this state
                top_combos = model_df.nlargest(top_n, prob_col).copy()

                for rank, (idx, row) in enumerate(top_combos.iterrows(), 1):
                    ranking_entry = {
                        'Model_Type': model_type,
                        'Target': target,
                        'Intention_Meaning': intention_states.get(target, target),
                        'State_Value': state,
                        'State_Label': state_label,
                        'Rank': rank,
                        'Probability': row[prob_col]
                    }

                    # Only add demographics that exist in THIS model
                    for demo_var in available_demos:  # Changed from demographic_vars.keys()
                        code_col = f'{demo_var}_Code'
                        if code_col in row.index and pd.notna(row[code_col]):
                            ranking_entry[demo_var] = row[code_col]
                            ranking_entry[f'{demo_var}_Label'] = demographic_vars[demo_var].get(
                                int(row[code_col]), 'Unknown'
                            )

                    rankings.append(ranking_entry)

    return pd.DataFrame(rankings)

# Update the function call
rankings_df = rank_top_combinations(results_df, demographic_vars, all_models_info, top_n=20)

print(f"\n✓ Generated rankings: {len(rankings_df):,} entries")
print(f"\nTop 10 overall:")
display(rankings_df.head(10))

# ============================================================================
# CRITICAL ANALYSIS TABLES FOR PRESENTATION
# ============================================================================
print("\nCreating Decision Tables for Presentation")
def create_presentation_tables(rankings_df):
    for model_type in rankings_df['Model_Type'].unique():
        for target in rankings_df['Target'].unique():
            print(f"\n{'='*110}")
            print(f"MODEL: {model_type} | TARGET: {target} - {intention_states.get(target, target)}")
            print(f"{'='*110}\n")

            model_target_df = rankings_df[
                (rankings_df['Model_Type'] == model_type) &
                (rankings_df['Target'] == target)
            ]

            for state_label in model_target_df['State_Label'].unique():
                state_df = model_target_df[
                    model_target_df['State_Label'] == state_label
                ].head(5)

                if len(state_df) == 0:
                    continue

                print(f"\n{state_label} (State {state_df['State_Value'].iloc[0]}):")
                print("-" * 80)

                # Create display table
                display_cols = ['Rank', 'Probability']
                # Select label columns, excluding 'State_Label' and dropping rows with NaN or 'Not Applicable' in those columns
                label_cols = [col for col in state_df.columns if col.endswith('_Label') and col != 'State_Label']
                # Only include label columns where there is at least one non-NaN and non-'Not Applicable' value in the top 5
                relevant_label_cols = [col for col in label_cols if (state_df[col].dropna() != 'Not Applicable').any()]

                display_cols.extend(relevant_label_cols)

                display_df = state_df[display_cols].copy()
                display_df['Probability'] = display_df['Probability'].round(4)

                # Drop rows where all relevant label columns are NaN or 'Not Applicable' (if that were possible based on the filter above)
                # In this case, we are already filtering which columns to display based on having valid data.

                print(display_df.to_string(index=False))

create_presentation_tables(rankings_df)

print("\n" + "="*110)
print("PART 2 COMPLETE: Conditional Probability Calculation")
print("="*110)

print("\n" + "="*110)
print("DEMOGRAPHIC VARIABLE COVERAGE BY MODEL")
print("="*110)

coverage_summary = []
for model_key, info in all_models_info.items():
    demo_vars = info['demographics']
    all_possible = set(demographic_vars.keys())
    missing = all_possible - set(demo_vars)

    coverage_summary.append({
        'Model': model_key,
        'Variables_Used': len(demo_vars),
        'Total_Available': len(all_possible),
        'Coverage_%': (len(demo_vars)/len(all_possible))*100,
        'Demographics': ', '.join(demo_vars)
    })

coverage_df = pd.DataFrame(coverage_summary)
display(coverage_df)
print("="*110)

"""###**Visualise the Results**

####**Creating Decision Tables as Heatmaps**
"""

# =============================================================================
# PART 4: VISUALIZE THE RESULTS
# =============================================================================

# Set style for professional visualizations
sns.set_style("whitegrid")
sns.set_context("talk")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']

print("\n" + "="*80)
print("PART 4: VISUALIZATION OF RESULTS")
print("="*80)

def create_decision_table_heatmap(rankings_df, model_type, target, top_n=10):
    """
    Create a HORIZONTAL heatmap showing top profiles for each intention state.
    """
    model_df = rankings_df[
        (rankings_df['Model_Type'] == model_type) &
        (rankings_df['Target'] == target) &
        (rankings_df['Rank'] <= top_n)
    ]

    if len(model_df) == 0:
        print(f"No data for {model_type} - {target}")
        return

    # Get unique states
    states = sorted(model_df['State_Label'].unique(),
                   key=lambda x: model_df[model_df['State_Label']==x]['State_Value'].iloc[0])

    # CHANGED: Vertical layout (rows = states, wider figure)
    fig, axes = plt.subplots(len(states), 1, figsize=(16, 4*len(states)))
    if len(states) == 1:
        axes = [axes]

    fig.suptitle(f'Top {top_n} Demographic Profiles\n{model_type} Model - {target} ({intention_states.get(target, target)})',
                 fontsize=18, fontweight='bold', y=0.995)

    for idx, state_label in enumerate(states):
        state_df = model_df[model_df['State_Label'] == state_label].head(top_n)

        # Create SHORTER profile descriptions
        profiles = []
        probabilities = []

        for _, row in state_df.iterrows():
            # Build compact profile string - ONE LINE per profile
            parts = []
            label_cols = [col for col in state_df.columns
                         if col.endswith('_Label') and col != 'State_Label']

            for col in label_cols:
                if pd.notna(row[col]) and row[col] != 'Not Applicable':
                    demo_name = col.replace('_Label', '').replace('_', ' ')
                    value = str(row[col])
                    # Abbreviate common long values
                    value = value.replace('Not Applicable', 'N/A')
                    value = value.replace('Employment Status', 'Emp')
                    value = value.replace('times per week', '/wk')
                    parts.append(f"{demo_name}: {value}")

            # Single line profile
            profile_str = ' | '.join(parts) if parts else f"Profile {row['Rank']}"
            profiles.append(profile_str)
            probabilities.append(row['Probability'])

        # CHANGED: Horizontal heatmap (probabilities as columns, profiles as rows)
        heatmap_data = np.array(probabilities).reshape(-1, 1)

        # Plot with better formatting
        ax = sns.heatmap(heatmap_data,
                        annot=True,
                        fmt='.4f',  # 4 decimal places
                        cmap='YlOrRd',
                        yticklabels=profiles,
                        xticklabels=['P(Intention | Demographics)'],
                        cbar_kws={'label': 'Probability', 'shrink': 0.8},
                        ax=axes[idx],
                        vmin=min(probabilities)*0.95,
                        vmax=max(probabilities)*1.02,
                        linewidths=0.5,
                        linecolor='gray',
                        annot_kws={'size': 11, 'weight': 'bold'})  # Larger annotation

        # Format title
        axes[idx].set_title(f'{state_label} (State {state_df["State_Value"].iloc[0]})',
                           fontsize=14, fontweight='bold', pad=15)
        axes[idx].set_ylabel('', fontsize=10)
        axes[idx].set_xlabel('')

        # Improve y-axis labels
        axes[idx].tick_params(axis='y', labelsize=9, rotation=0)
        axes[idx].tick_params(axis='x', labelsize=11)

    plt.tight_layout()
    plt.show()
    print("\n" + "="*180)

 # Create decision tables for all models and targets
for model_type in rankings_df['Model_Type'].unique():
    for target in rankings_df['Target'].unique():
        create_decision_table_heatmap(rankings_df, model_type, target, top_n=10)

# -----------------------------------------------------------------------------
# HORIZONTAL BAR CHARTS
# -----------------------------------------------------------------------------
print("\nCreating Bar Chart Visualizations")

def create_decision_bar_chart(rankings_df, model_type, target, top_n=10):
    """
    Create clean horizontal bar charts for better readability.
    Perfect for PowerPoint presentations.
    """
    model_df = rankings_df[
        (rankings_df['Model_Type'] == model_type) &
        (rankings_df['Target'] == target) &
        (rankings_df['Rank'] <= top_n)
    ]

    if len(model_df) == 0:
        print(f"No data for {model_type} - {target}")
        return

    # Get unique states sorted by value
    states = sorted(model_df['State_Label'].unique(),
                   key=lambda x: model_df[model_df['State_Label']==x]['State_Value'].iloc[0])

    # Create subplot layout
    fig, axes = plt.subplots(1, len(states), figsize=(7*len(states), 10))
    if len(states) == 1:
        axes = [axes]

    fig.suptitle(f'Top {top_n} Demographic Profiles\n{model_type} Model - {target} ({intention_states.get(target, target)})',
                 fontsize=18, fontweight='bold', y=0.98)

    for idx, state_label in enumerate(states):
        state_df = model_df[model_df['State_Label'] == state_label].head(top_n).copy()

        # Sort by probability for better visualization
        state_df = state_df.sort_values('Probability', ascending=True)

        # Create compact profile labels
        labels = []
        for _, row in state_df.iterrows():
            parts = []
            label_cols = [col for col in state_df.columns
                         if col.endswith('_Label') and col != 'State_Label']

            for col in label_cols:
                if pd.notna(row[col]) and row[col] != 'Not Applicable':
                    value = str(row[col])
                    # Abbreviate long values
                    value = value.replace('times per week', '/wk')
                    value = value.replace('Prefer not to say', 'Prefer N/S')
                    value = value.replace('Employment Status', 'Emp')
                    value = value.replace('Level of Education', 'Edu')
                    parts.append(value)

            # Multi-line label
            label_str = '\n'.join(parts) if parts else f"Profile {int(row['Rank'])}"
            labels.append(label_str)

        probabilities = state_df['Probability'].values

        # Create horizontal bar chart with gradient colors
        colors = plt.cm.YlOrRd(np.linspace(0.4, 0.9, len(probabilities)))
        bars = axes[idx].barh(range(len(labels)), probabilities,
                             color=colors,
                             edgecolor='black', linewidth=1)

        # Add probability values on bars
        for i, (bar, prob) in enumerate(zip(bars, probabilities)):
            width = bar.get_width()
            axes[idx].text(width + (max(probabilities) - min(probabilities)) * 0.01,
                          i, f'{prob:.4f}',
                          va='center', ha='left',
                          fontsize=11, fontweight='bold')

        # Format axes
        axes[idx].set_yticks(range(len(labels)))
      https://colab.research.google.com/drive/1Ji5260NS44Rn8m10TMljRBtLL289JEbV?authuser=1
        axes[idx].set_xlabel('Probability', fontsize=12, fontweight='bold')
        axes[idx].set_title(f'{state_label}\n(State {int(state_df["State_Value"].iloc[0])})',
                           fontsize=14, fontweight='bold', pad=15)
        axes[idx].grid(axis='x', alpha=0.3, linestyle='--')
        axes[idx].set_xlim(probabilities.min()*0.95, probabilities.max()*1.12)

        # Add subtle background
        axes[idx].set_facecolor('#f8f9fa')

    plt.tight_layout()
    plt.show()
    plt.close()
    print("\n" + "="*180)

# Generate bar charts for all models
print("\nGenerating bar chart visualizations...")
for model_type in rankings_df['Model_Type'].unique():
    for target in rankings_df['Target'].unique():
        create_decision_bar_chart(rankings_df, model_type, target, top_n=10)

print(f"\nBar charts complete")

"""####**Probability Distribution Comparison (Balanced vs Unbalanced)**"""

# -----------------------------------------------------------------------------
# Probability Distribution Comparison (Balanced vs Unbalanced)
# -----------------------------------------------------------------------------
print("\nComparing Probability Distributions")

def compare_probability_distributions(results_df):
    """
    Compare how probabilities differ between balanced and unbalanced models.
    KEY VISUALIZATION for showing impact of balancing techniques.
    """
    # Get all probability columns
    prob_cols = [col for col in results_df.columns if col.startswith('P_')]

    # Group by model type
    model_types = results_df['Model_Type'].unique()

    for target in results_df['Target'].unique():
        target_df = results_df[results_df['Target'] == target]
        target_prob_cols = [col for col in prob_cols if target in col]

        if len(target_prob_cols) == 0:
            continue

        fig, axes = plt.subplots(len(target_prob_cols), len(model_types),
                                figsize=(6*len(model_types), 4*len(target_prob_cols)))

        if len(target_prob_cols) == 1 and len(model_types) == 1:
            axes = np.array([[axes]])
        elif len(target_prob_cols) == 1:
            axes = axes.reshape(1, -1)
        elif len(model_types) == 1:
            axes = axes.reshape(-1, 1)

        fig.suptitle(f'Probability Distributions: {target} - {intention_states.get(target, target)}',
                    fontsize=16, y=0.995)

        for i, prob_col in enumerate(target_prob_cols):
            state = prob_col.split('_State_')[1]
            state_label = intention_values.get(int(state), state)

            for j, model_type in enumerate(model_types):
                model_data = target_df[target_df['Model_Type'] == model_type][prob_col]

                # Create distribution plot
                axes[i, j].hist(model_data, bins=50, alpha=0.7, color='steelblue',
                               edgecolor='black', density=True)

                # Add KDE
                model_data.plot(kind='kde', ax=axes[i, j], color='red', linewidth=2)

                # Add statistics
                mean_val = model_data.mean()
                median_val = model_data.median()
                axes[i, j].axvline(mean_val, color='darkred', linestyle='--',
                                  linewidth=2, label=f'Mean: {mean_val:.4f}')
                axes[i, j].axvline(median_val, color='darkgreen', linestyle='--',
                                  linewidth=2, label=f'Median: {median_val:.4f}')

                # Labels
                if j == 0:
                    axes[i, j].set_ylabel(f'{state_label}\nDensity', fontsize=10)
                if i == 0:
                    axes[i, j].set_title(f'{model_type} Model', fontsize=12)
                if i == len(target_prob_cols) - 1:
                    axes[i, j].set_xlabel('Probability', fontsize=10)

                axes[i, j].legend(fontsize=8)
                axes[i, j].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()
        plt.close()
        print("\n" + "="*180)

compare_probability_distributions(results_df)

"""####**Highlighting Probability Shifts Between Models**

The code below generates bar charts to visualize how the probabilities of the top 10 demographic profiles for each purchase intention state change across different Bayesian network models (Original vs. SMOTE, ML, GAN).

Each chart shows the top profiles for a specific intention state. Within each profile's bar group, you can compare the probability predicted by each model type. Annotations highlight significant differences (shifts) in probability between the Original model and the balanced models. This helps to understand the impact of data balancing techniques on the model's probabilistic inference for key customer segments.
"""

# -----------------------------------------------------------------------------
# Highlighting Probability Shifts Between Models
# -----------------------------------------------------------------------------
print("\nProbability Shifts: Balanced vs Unbalanced")

def visualize_probability_shifts(rankings_df, top_n=10):
    """
    Show how top profile probabilities change between balanced and unbalanced models.
    CRITICAL for demonstrating impact of synthetic data.
    """
    for target in rankings_df['Target'].unique():
        target_df = rankings_df[rankings_df['Target'] == target]

        states = sorted(target_df['State_Label'].unique(),
                       key=lambda x: target_df[target_df['State_Label']==x]['State_Value'].iloc[0])

        # Adjust figure size for better readability with multiple states
        fig, axes = plt.subplots(len(states), 1, figsize=(16, 6*len(states))) # Increased height
        if len(states) == 1:
            axes = [axes]

        fig.suptitle(f'Probability Shifts Between Models for {target} ({intention_states.get(target, target)})', # More descriptive title
                    fontsize=18, fontweight='bold', y=0.995)

        for idx, state_label in enumerate(states):
            state_df = target_df[
                (target_df['State_Label'] == state_label) &
                (target_df['Rank'] <= top_n)
            ].copy() # Use .copy() to avoid SettingWithCopyWarning

            if len(state_df) == 0:
                continue

            # Ensure all model types are present, fill missing with 0 for pivoting
            all_model_types = rankings_df['Model_Type'].unique()
            for m_type in all_model_types:
                if m_type not in state_df['Model_Type'].unique():
                    # Add dummy row to ensure column exists in pivot
                    dummy_row = {col: None for col in state_df.columns}
                    dummy_row['Model_Type'] = m_type
                    dummy_row['Target'] = target
                    dummy_row['State_Label'] = state_label
                    dummy_row['Rank'] = 1 # Arbitrary rank for pivot structure
                    dummy_df = pd.DataFrame([dummy_row])
                    state_df = pd.concat([state_df, dummy_df], ignore_index=True)


            # Pivot for comparison
            comparison_data = state_df.pivot_table(
                index='Rank',
                columns='Model_Type',
                values='Probability'
            )
            # Reorder columns to have 'Original' first if it exists
            if 'Original' in comparison_data.columns:
                cols = ['Original'] + [col for col in comparison_data.columns if col != 'Original']
                comparison_data = comparison_data[cols]


            # Plot
            comparison_data.plot(kind='bar', ax=axes[idx], width=0.8)
            axes[idx].set_title(f'Intention State: "{state_label}" (State {int(state_df["State_Value"].iloc[0])})', # Clearer state title
                               fontsize=15, fontweight='bold', pad=10)
            axes[idx].set_xlabel('Profile Rank', fontsize=12, fontweight='bold')
            axes[idx].set_ylabel('Probability', fontsize=12, fontweight='bold') # More descriptive Y-label
            axes[idx].legend(title='Model Type', fontsize=10)
            axes[idx].grid(axis='y', alpha=0.3)
            axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=0)

            # Add difference annotations
            if len(comparison_data.columns) >= 2 and 'Original' in comparison_data.columns:
                # Compare 'Original' to other models
                original_col = 'Original'
                other_cols = [col for col in comparison_data.columns if col != original_col]

                for rank in comparison_data.index:
                     original_prob = comparison_data.loc[rank, original_col]
                     annotation_offset = 0 # Initialize offset for staggering annotations

                     if pd.isna(original_prob): continue # Skip if Original data is missing

                     for other_col in other_cols:
                        other_prob = comparison_data.loc[rank, other_col]
                        if pd.notna(other_prob):
                            diff = other_prob - original_prob
                            if abs(diff) > 0.005:  # Show differences greater than 0.5%
                                # Find the x position for the annotation (center of the two bars)
                                # This is a bit tricky with grouped bars, roughly center on the group
                                bar_width = axes[idx].patches[0].get_width() # Assuming bar width is consistent
                                original_bar_x = axes[idx].patches[(rank-1)*len(comparison_data.columns)].get_x()
                                other_bar_x = axes[idx].patches[(rank-1)*len(comparison_data.columns) + comparison_data.columns.get_loc(other_col)].get_x()

                                x_pos = (original_bar_x + other_bar_x + bar_width) / 2
                                # Stagger the y-position slightly for multiple annotations on the same rank
                                y_pos = max(original_prob, other_prob) + (axes[idx].get_ylim()[1] - axes[idx].get_ylim()[0]) * (0.01 + annotation_offset)
                                annotation_offset += 0.05 # Increase offset for the next annotation

                                axes[idx].text(rank-1, y_pos, f'Δ{diff:+.3f} ({other_col})', # Add model name to diff
                                             ha='center', va='bottom',
                                             fontsize=9, color='red', fontweight='bold')


        plt.tight_layout()
        plt.show()
        plt.close()
        print("\n" + "="*180)

# Execute the visualization function
visualize_probability_shifts(rankings_df, top_n=10)

print(f"\n✓ Probability shift visualizations complete")

"""#### **Inference Trees / Decision Paths**"""

# -----------------------------------------------------------------------------
#  Inference Trees / Decision Paths
# -----------------------------------------------------------------------------
print("\nCreating Inference Trees")

# Import Rectangle patch
from matplotlib.patches import Rectangle


def create_inference_tree_visualization(rankings_df, model_type, target, state_label, top_n=5):
    """
    Visualize decision paths for top profiles in a tree-like structure.
    """
    tree_df = rankings_df[
        (rankings_df['Model_Type'] == model_type) &
        (rankings_df['Target'] == target) &
        (rankings_df['State_Label'] == state_label) &
        (rankings_df['Rank'] <= top_n)
    ]

    if len(tree_df) == 0:
        return

    fig, ax = plt.subplots(figsize=(16, 10))
    fig.suptitle(f'Decision Paths: Top {top_n} Profiles\n{model_type} - {target} - {intention_values.get(int(tree_df["State_Value"].iloc[0]), state_label)}', # Corrected title
                fontsize=14, y=0.98)

    # Get demographic label columns
    label_cols = [col for col in tree_df.columns if col.endswith('_Label') and col != 'State_Label']

    y_position = top_n

    for idx, (_, row) in enumerate(tree_df.iterrows()):
        # Main node - rank and probability
        main_text = f"Rank {row['Rank']}\nP = {row['Probability']:.4f}"
        ax.add_patch(Rectangle((0, y_position-0.4), 0.15, 0.8,
                               facecolor='lightcoral', edgecolor='black', linewidth=2))
        ax.text(0.075, y_position, main_text, ha='center', va='center',
               fontsize=10, fontweight='bold')

        # Demographic branches
        x_pos = 0.25
        for col in label_cols:
            if pd.notna(row[col]):
                demo_name = col.replace('_Label', '').replace('_', ' ')
                demo_value = str(row[col])

                # Draw connection line
                ax.plot([0.15, x_pos], [y_position, y_position], 'k-', linewidth=1.5)

                # Draw demographic node
                ax.add_patch(Rectangle((x_pos, y_position-0.3), 0.18, 0.6,
                                      facecolor='lightblue', edgecolor='black', linewidth=1.5))

                # Add text
                node_text = f"{demo_name}\n{demo_value}"
                ax.text(x_pos + 0.09, y_position, node_text, ha='center', va='center',
                       fontsize=8, wrap=True)

                x_pos += 0.20

        y_position -= 1.2

    ax.set_xlim(-0.05, x_pos + 0.1)
    ax.set_ylim(-0.5, top_n + 0.5)
    ax.axis('off')

    plt.tight_layout()
    plt.show()
    plt.close()
    print("\n" + "="*180)

# Create inference trees for key scenarios
for model_type in rankings_df['Model_Type'].unique()[:2]:  # Limit to 2 models
    for target in rankings_df['Target'].unique()[:2]:  # Limit to 2 targets
        # Find a valid state_label for the target
        valid_state_labels = rankings_df[(rankings_df['Model_Type'] == model_type) &
                                          (rankings_df['Target'] == target)]['State_Label'].unique()
        if len(valid_state_labels) > 0:
            state_label_to_use = valid_state_labels[0] # Use the first available state label
            create_inference_tree_visualization(rankings_df, model_type, target, state_label_to_use, top_n=5)
        else:
            print(f"  ⚠ No data for {model_type} - {target} to generate inference tree.")

"""# **Part 3: Results and Interpretation**

Now that we have our probabilities compiled in a DataFrame, we can visualize and interpret them. The best way to compare these results is with a **heatmap**.

A heatmap will allow us to instantly see which demographic profiles have the highest probability for each purchase intention. We will generate separate heatmaps for each model and each intention type to make our comparisons clear and insightful.

###**Pattern Interpretation**
"""

# -----------------------------------------------------------------------------
# Pattern Interpretation
# -----------------------------------------------------------------------------
print("\nPattern Analysis and Interpretation")

def analyze_demographic_patterns(rankings_df, demographic_vars):
    """
    Identify and interpret patterns in top demographic profiles.
    """
    patterns = []

    for target in rankings_df['Target'].unique():
        for state_label in rankings_df[rankings_df['Target'] == target]['State_Label'].unique():
            subset = rankings_df[
                (rankings_df['Target'] == target) &
                (rankings_df['State_Label'] == state_label) &
                (rankings_df['Rank'] <= 10)
            ]

            if len(subset) == 0:
                continue

            # Analyze each demographic variable
            for demo_var in demographic_vars.keys():
                label_col = f'{demo_var}_Label'
                if label_col in subset.columns:
                    value_counts = subset[label_col].value_counts()

                    if len(value_counts) > 0:
                        most_common = value_counts.index[0]
                        frequency = value_counts.iloc[0]
                        percentage = (frequency / len(subset)) * 100

                        patterns.append({
                            'Target': target,
                            'Intention': intention_states.get(target, target),
                            'State': state_label,
                            'Demographic': demo_var,
                            'Most_Common_Value': most_common,
                            'Frequency': frequency,
                            'Percentage': percentage,
                            'Total_Profiles': len(subset)
                        })

    return pd.DataFrame(patterns)

patterns_df = analyze_demographic_patterns(rankings_df, demographic_vars)

print("\nKey Patterns Identified")
print("\nMost Predictive Demographics (appearing in >50% of top profiles):")
high_predictive = patterns_df[patterns_df['Percentage'] > 50].sort_values('Percentage', ascending=False)
display(high_predictive.head(20))

"""###**Compare Networks**"""

# -----------------------------------------------------------------------------
#  Compare Networks
# -----------------------------------------------------------------------------
print("\nCritical Comparison of Networks")

def compare_balanced_vs_unbalanced(rankings_df, results_df):
    """
    Comprehensive comparison between balanced and unbalanced models.
    This is CRITICAL for the 15 marks on comparison.
    """
    comparison_insights = []

    # 1. Compare top profile consistency
    print("\n1. TOP PROFILE CONSISTENCY:")
    print("-" * 80)

    for target in rankings_df['Target'].unique():
        for state_label in rankings_df[rankings_df['Target'] == target]['State_Label'].unique():

            # Get top 5 from each model type
            model_types = rankings_df['Model_Type'].unique()

            if len(model_types) < 2:
                continue

            top_profiles = {}
            for model_type in model_types:
                subset = rankings_df[
                    (rankings_df['Model_Type'] == model_type) &
                    (rankings_df['Target'] == target) &
                    (rankings_df['State_Label'] == state_label) &
                    (rankings_df['Rank'] <= 5)
                ]

                # Create profile signatures
                profiles = []
                for _, row in subset.iterrows():
                    sig = tuple(row[col] for col in demographic_vars.keys() if col in row.index)
                    profiles.append(sig)

                top_profiles[model_type] = set(profiles)

            # Calculate overlap
            if len(top_profiles) >= 2:
                model_list = list(top_profiles.keys())
                overlap = len(top_profiles[model_list[0]] & top_profiles[model_list[1]])
                union = len(top_profiles[model_list[0]] | top_profiles[model_list[1]])
                jaccard = overlap / union if union > 0 else 0

                insight = {
                    'Target': target,
                    'State': state_label,
                    'Comparison': f"{model_list[0]} vs {model_list[1]}",
                    'Overlap': overlap,
                    'Total_Unique': union,
                    'Jaccard_Similarity': jaccard,
                    'Consistency': 'High' if jaccard > 0.6 else 'Medium' if jaccard > 0.3 else 'Low'
                }
                comparison_insights.append(insight)

                print(f"\n{target} - {state_label}:")
                print(f"  {model_list[0]} vs {model_list[1]}")
                print(f"  Common profiles: {overlap}/5")
                print(f"  Similarity: {jaccard:.2%}")
                print(f"  Consistency: {insight['Consistency']}")

    comparison_df = pd.DataFrame(comparison_insights)

    # 2. Compare probability distributions
    print("\n\n2. PROBABILITY DISTRIBUTION DIFFERENCES:")
    print("-" * 80)

    prob_comparison = []

    for target in results_df['Target'].unique():
        target_df = results_df[results_df['Target'] == target]
        prob_cols = [col for col in target_df.columns if col.startswith(f'P_{target}_')]

        model_types = target_df['Model_Type'].unique()

        for prob_col in prob_cols:
            state = prob_col.split('_State_')[1]

            stats_by_model = {}
            for model_type in model_types:
                model_data = target_df[target_df['Model_Type'] == model_type][prob_col]
                stats_by_model[model_type] = {
                    'mean': model_data.mean(),
                    'std': model_data.std(),
                    'max': model_data.max(),
                    'min': model_data.min()
                }

            # Compare statistics
            if len(stats_by_model) >= 2:
                models = list(stats_by_model.keys())
                mean_diff = stats_by_model[models[1]]['mean'] - stats_by_model[models[0]]['mean']
                std_diff = stats_by_model[models[1]]['std'] - stats_by_model[models[0]]['std']

                prob_comparison.append({
                    'Target': target,
                    'State': state,
                    'Model_1': models[0],
                    'Model_2': models[1],
                    'Mean_Diff': mean_diff,
                    'Std_Diff': std_diff,
                    'Interpretation': 'Balanced model increases probability' if mean_diff > 0 else 'Unbalanced model increases probability'
                })

                print(f"\n{target} - State {state}:")
                print(f"  Mean difference: {mean_diff:+.4f}")
                print(f"  Std difference: {std_diff:+.4f}")

    prob_comparison_df = pd.DataFrame(prob_comparison)

    # 3. Disproportionate predictiveness
    print("\n\n3. DISPROPORTIONATELY PREDICTIVE DEMOGRAPHICS:")
    print("-" * 80)

    disproportion_analysis = []

    for demo_var in demographic_vars.keys():
        label_col = f'{demo_var}_Label'
        if label_col not in rankings_df.columns:
            continue

        # Compare frequency across model types
        for model_type in rankings_df['Model_Type'].unique():
            top_10 = rankings_df[
                (rankings_df['Model_Type'] == model_type) &
                (rankings_df['Rank'] <= 10)
            ]

            if label_col in top_10.columns:
                freq = top_10[label_col].value_counts(normalize=True).to_dict()

                for value, proportion in freq.items():
                    disproportion_analysis.append({
                        'Model_Type': model_type,
                        'Demographic': demo_var,
                        'Value': value,
                        'Proportion_in_Top_10': proportion
                    })

    disproportion_df = pd.DataFrame(disproportion_analysis)

    # Find demographics with large differences between models
    if len(disproportion_df) > 0:
        pivot = disproportion_df.pivot_table(
            index=['Demographic', 'Value'],
            columns='Model_Type',
            values='Proportion_in_Top_10',
            fill_value=0
        )

        if pivot.shape[1] >= 2:
            pivot['Difference'] = abs(pivot.iloc[:, 0] - pivot.iloc[:, 1])
            high_diff = pivot[pivot['Difference'] > 0.2].sort_values('Difference', ascending=False)

            print("\nDemographics with >20% difference between models:")
            display(high_diff)

    return comparison_df, prob_comparison_df, disproportion_df

consistency_df, prob_comp_df, dispro_df = compare_balanced_vs_unbalanced(rankings_df, results_df)

"""### **Bias Analysis**
1. **BIAS IN ORIGINAL (UNBALANCED) DATASET:**
---
   a) Class Imbalance Bias:
      - Original data has unequal representation of purchase intentions
      - Majority class dominates model learning
      - Minority classes are under-represented in predictions
      - Model becomes biased toward predicting majority class

   b) Sampling Bias:
      - Certain demographic combinations are rare or missing
      - Model has limited exposure to diverse customer profiles
      - Predictions for rare profiles are unreliable

   c) Impact on Business Decisions:
      - Marketing strategies favor majority customer segments
      - Minority customer segments are ignored or misunderstood
      - Missed opportunities for targeted campaigns

   d) Statistical Issues:
      - High accuracy on majority class, poor on minority
      - Misleading performance metrics
      - Overfitting to prevalent patterns

2. **BIAS INTRODUCED BY BALANCED DATASETS:**
---

   a) Artificial Distribution Bias:
      - SMOTE creates synthetic samples through interpolation
      - May create unrealistic demographic combinations
      - Equal class weights don't reflect real-world proportions

   b) Over-representation of Minority Classes:
      - Synthetic data amplifies patterns that may not exist
      - False confidence in rare scenarios
      - Model may predict minority classes more than reality warrants

   c) Loss of True Distribution Information:
      - Original class proportions are ignored
      - Difficulty knowing if high probability reflects reality or synthesis

   d) Interpolation Artifacts:
      - SMOTE creates points between existing samples
      - May introduce impossible or highly unlikely combinations
      - Can create "fantasy customers" that don't exist

   e) Different Biases by Technique:
      - SMOTE: Interpolation artifacts, boundary issues
      - GAN: Mode collapse, generated samples may be unrealistic
      - Each technique introduces its own characteristic biases

3. **TRADE-OFFS AND RECOMMENDATIONS:**
---

   a) Use Balanced Models For:
      - Exploratory analysis of minority classes
      - Identifying potential high-value customer segments
      - Developing hypotheses about rare profiles
      - Ensuring all intention types are considered

   b) Use Unbalanced Models For:
      - Real-world probability estimates
      - Resource allocation decisions
      - When true class proportions matter
      - Conservative business strategies

   c) Best Practice - Use Both:
      - Compare predictions between balanced and unbalanced
      - Balanced models identify opportunities
      - Unbalanced models provide realistic expectations
      - Combined insights lead to better decisions

   d) Critical Considerations:
      - Always disclose which model type was used
      - Interpret probabilities in context of data source
      - Validate synthetic data patterns with domain experts
      - Monitor real-world outcomes to assess model validity

###**Real-World Implications and Recommendations**

####**Analyze Demographic Patterns from Rankings**
"""

# -----------------------------------------------------------------------------
# Step 1: Analyze Demographic Patterns from Rankings
# -----------------------------------------------------------------------------
print("\n[Step 1] Analyzing Demographic Patterns")

def extract_demographic_patterns(rankings_df):
    """
    Extract the most common demographic patterns from top-ranked profiles.
    """
    patterns = []

    for model_type in rankings_df['Model_Type'].unique():
        for target in rankings_df['Target'].unique():
            for state_label in rankings_df['State_Label'].unique():
                # Get top 10 profiles for this combination
                subset = rankings_df[
                    (rankings_df['Model_Type'] == model_type) &
                    (rankings_df['Target'] == target) &
                    (rankings_df['State_Label'] == state_label) &
                    (rankings_df['Rank'] <= 10)
                ]

                if len(subset) == 0:
                    continue

                state_value = subset['State_Value'].iloc[0]

                # Analyze each demographic variable
                label_cols = [col for col in subset.columns
                             if col.endswith('_Label') and col != 'State_Label']

                for col in label_cols:
                    # Get non-NA values
                    values = subset[col].dropna()
                    values = values[values != 'Not Applicable']

                    if len(values) == 0:
                        continue

                    # Find most common value
                    value_counts = values.value_counts()
                    most_common = value_counts.index[0]
                    count = value_counts.iloc[0]
                    percentage = (count / len(subset)) * 100

                    # Get average probability for this pattern
                    avg_prob = subset[subset[col] == most_common]['Probability'].mean()

                    patterns.append({
                        'Model_Type': model_type,
                        'Target': target,
                        'Intention_Meaning': intention_states.get(target, target),
                        'State_Value': state_value,
                        'State_Label': state_label,
                        'Demographic': col.replace('_Label', '').replace('_', ' '),
                        'Most_Common_Value': most_common,
                        'Frequency': count,
                        'Percentage': percentage,
                        'Avg_Probability': avg_prob,
                        'Total_Profiles': len(subset)
                    })

    return pd.DataFrame(patterns)

patterns_df = extract_demographic_patterns(rankings_df)
print(f"Extracted {len(patterns_df):,} demographic patterns")
print(f"\nSample patterns:")
display(patterns_df.head(10))

"""####**Generate Customer Segmentation Insights**"""

# -----------------------------------------------------------------------------
# Step 2: Generate Customer Segmentation Insights
# -----------------------------------------------------------------------------
print("\n[Step 2] Customer Segmentation Analysis")

def create_customer_segments(patterns_df, rankings_df):
    """
    Create distinct customer segments based on purchase intention patterns.
    """
    segments = {}

    # For each purchase intention, identify the strongest segments
    for target in patterns_df['Target'].unique():
        intention = intention_states.get(target, target)

        print(f"\n{'='*80}")
        print(f"SEGMENTATION FOR {target}: {intention}")
        print('='*80)

        # Focus on "Agree" and "Strongly Agree" states (positive intentions)
        positive_states = patterns_df[
            (patterns_df['Target'] == target) &
            (patterns_df['State_Value'].isin([4, 5]))
        ]

        if len(positive_states) == 0:
            print("  No positive intention patterns found")
            continue

        # Group by demographic to find strongest predictors
        demo_summary = positive_states.groupby('Demographic').agg({
            'Percentage': 'mean',
            'Avg_Probability': 'mean',
            'Most_Common_Value': lambda x: x.value_counts().index[0] if len(x) > 0 else 'N/A'
        }).sort_values('Percentage', ascending=False)

        print(f"\nKey Demographic Predictors (for positive intentions):")
        print(demo_summary.head(5).to_string())

        # Create segment profile
        segment_profile = {}
        for demo in demo_summary.head(3).index:
            most_common = demo_summary.loc[demo, 'Most_Common_Value']
            percentage = demo_summary.loc[demo, 'Percentage']
            segment_profile[demo] = {
                'value': most_common,
                'prevalence': percentage
            }

        segments[target] = {
            'intention': intention,
            'profile': segment_profile,
            'avg_probability': demo_summary['Avg_Probability'].mean()
        }

        print(f"\n✓ Segment Profile Created:")
        for demo, info in segment_profile.items():
            print(f"  • {demo}: {info['value']} ({info['prevalence']:.1f}% of top profiles)")

    return segments

segments = create_customer_segments(patterns_df, rankings_df)

"""####**Generate Targeted Marketing Recommendations**"""

# -----------------------------------------------------------------------------
# Step 3: Generate Targeted Marketing Recommendations
# -----------------------------------------------------------------------------
print("\n[Step 3] Targeted Marketing Recommendations")

def generate_marketing_recommendations(segments, rankings_df):
    """
    Generate specific, actionable marketing recommendations.
    """
    recommendations = []

    # Marketing strategies specific to YOUR purchase intentions
    marketing_strategies = {
        'PI1': {  # I intend to purchase from this grocery store
            'objective': 'Convert browsing intent into immediate purchase',
            'tactics': [
                'In-store promotions at point of decision',
                'Mobile app push notifications for current shoppers',
                'Digital coupons activated at store entry',
                'Loyalty program incentives for immediate purchase'
            ]
        },
        'PI2': {  # I would like to repeat my experience
            'objective': 'Build loyalty and encourage return visits',
            'tactics': [
                'Post-visit satisfaction surveys with incentives',
                'Membership/subscription programs',
                'Personalized "We miss you" campaigns',
                'Exclusive member-only shopping hours or events'
            ]
        },
        'PI3': {  # I would purchase from this grocery store in the future
            'objective': 'Convert future intent into near-term action',
            'tactics': [
                'Email reminder campaigns with shopping lists',
                'Seasonal promotional calendars',
                'Future purchase discount vouchers',
                'Product availability notifications'
            ]
        },
        'PI4': {  # I would recommend purchasing in this grocery store to others
            'objective': 'Activate word-of-mouth and referral marketing',
            'tactics': [
                'Referral reward programs',
                'Social media sharing incentives',
                'Customer testimonial collection',
                'Brand ambassador programs'
            ]
        }
    }

    for target, segment_info in segments.items():
        intention = segment_info['intention']
        profile = segment_info['profile']

        print(f"\n{'='*80}")
        print(f"{target}: {intention}")
        print('='*80)

        # Get strategy
        strategy_info = marketing_strategies.get(target, {
            'objective': 'Engage target customers',
            'tactics': ['Generic marketing campaigns']
        })

        # Build recommendation
        rec = {
            'Target': target,
            'Intention': intention,
            'Objective': strategy_info['objective']
        }

        print(f"\nTARGET SEGMENT PROFILE:")
        demo_descriptions = []
        for demo, info in profile.items():
            desc = f"  • {demo}: {info['value']}"
            print(desc)
            demo_descriptions.append(f"{demo} = {info['value']}")

        rec['Segment_Description'] = ', '.join(demo_descriptions)

        print(f"\nMARKETING OBJECTIVE:")
        print(f"  {strategy_info['objective']}")

        print(f"\nRECOMMENDED TACTICS:")
        for i, tactic in enumerate(strategy_info['tactics'], 1):
            print(f"  {i}. {tactic}")

        rec['Tactics'] = strategy_info['tactics']

        # Add channel recommendations based on demographics
        print(f"\nRECOMMENDED CHANNELS:")
        channels = []

        # Age-based channel selection
        age_pattern = next((v['value'] for k, v in profile.items() if 'Age' in k), None)
        if age_pattern:
            if any(young in age_pattern for young in ['18-22', '23-28']):
                channels.extend(['Instagram', 'TikTok', 'Mobile App'])
                print("  • Social Media (Instagram, TikTok)")
                print("  • Mobile-first campaigns")
            elif any(mid in age_pattern for mid in ['29-35', '35-49']):
                channels.extend(['Facebook', 'Email', 'Mobile App'])
                print("  • Facebook advertising")
                print("  • Email marketing")
            else:  # Older demographics
                channels.extend(['Email', 'In-store', 'Direct Mail'])
                print("  • Email newsletters")
                print("  • In-store promotions")
                print("  • Direct mail campaigns")

        # Shopping frequency consideration
        freq_pattern = next((v['value'] for k, v in profile.items() if 'Shopping frequency' in k), None)
        if freq_pattern and any(high in freq_pattern for high in ['6-7', '5-6']):
            channels.append('Loyalty App')
            print("  • Loyalty program integration (high-frequency shoppers)")

        rec['Channels'] = channels

        # Add comparison insights (Original vs Synthetic data)
        print(f"\nMODEL COMPARISON INSIGHTS:")
        target_rankings = rankings_df[rankings_df['Target'] == target]

        for model_type in ['Original', 'SMOTE', 'ML', 'GAN']:
            model_data = target_rankings[
                (target_rankings['Model_Type'] == model_type) &
                (target_rankings['State_Value'].isin([4, 5]))  # Positive intentions
            ]
            if len(model_data) > 0:
                avg_prob = model_data['Probability'].mean()
                print(f"  • {model_type}: Average probability = {avg_prob:.4f}")

        recommendations.append(rec)

        print("\n")

    return pd.DataFrame(recommendations)

recommendations_df = generate_marketing_recommendations(segments, rankings_df)

"""####**Bias Analysis and Warnings**"""

# -----------------------------------------------------------------------------
# Step 4: Bias Analysis and Warnings
# -----------------------------------------------------------------------------
print("\n[Step 4] Bias Analysis and Limitations")

def analyze_biases(rankings_df, patterns_df):
    """
    Identify potential biases in the analysis.
    """
    print(f"\n{'='*80}")
    print("POTENTIAL BIASES AND LIMITATIONS")
    print('='*80)

    biases = []

    # 1. Sample size bias
    print("\n1. SMALL DATASET BIAS:")
    print("-" * 80)

    # Check probability distributions
    prob_ranges = rankings_df.groupby(['Model_Type', 'Target'])['Probability'].agg(['min', 'max', 'std'])

    print("\nProbability Range Analysis:")
    print("(Small ranges suggest limited discriminative power)\n")
    for idx, row in prob_ranges.iterrows():
        range_size = row['max'] - row['min']
        print(f"  {idx[0]} - {idx[1]}: Range = {range_size:.4f}, Std = {row['std']:.4f}")

        if range_size < 0.15:
            bias_note = f"{idx[0]} {idx[1]}: Limited probability range suggests weak demographic signals"
            print(f"    {bias_note}")
            biases.append(bias_note)

    # 2. Synthetic data bias
    print("\n\n2. SYNTHETIC DATA BIAS:")
    print("-" * 80)

    synthetic_models = ['SMOTE', 'ML', 'GAN']
    original_probs = rankings_df[rankings_df['Model_Type'] == 'Original']['Probability']

    print("\nDifferences introduced by synthetic data:\n")
    for model in synthetic_models:
        synthetic_probs = rankings_df[rankings_df['Model_Type'] == model]['Probability']
        if len(synthetic_probs) > 0:
            diff = synthetic_probs.mean() - original_probs.mean()
            print(f"  {model} vs Original: Mean difference = {diff:+.4f}")

            if abs(diff) > 0.01:
                bias_note = f"{model} shows {abs(diff):.2%} probability shift - may introduce artificial patterns"
                print(f"    {bias_note}")
                biases.append(bias_note)

    # 3. Variable selection bias
    print("\n\n3. VARIABLE SELECTION BIAS:")
    print("-" * 80)

    print("\nDemographics missing from models:\n")
    all_demographics = set(demographic_vars.keys())

    for model_key, info in all_models_info.items():
        model_demos = set(info['demographics'])
        missing_demos = all_demographics - model_demos

        if missing_demos:
            print(f"  {model_key}: Missing {len(missing_demos)}/{len(all_demographics)} variables")
            print(f"    Not included: {', '.join(missing_demos)}")

            bias_note = f"⚠️ {model_key}: Incomplete demographic profile limits segmentation insights"
            biases.append(bias_note)

    # 4. Class imbalance effects
    print("\n\n4. CLASS IMBALANCE EFFECTS:")
    print("-" * 80)

    print("\nOriginal dataset likely had imbalanced purchase intentions")
    print("(This is why synthetic data was needed)\n")

    print("Implications:")
    print("  • Original model may be biased toward majority class")
    print("  • Rare demographic combinations may be underrepresented")
    print("  • Synthetic data helps but introduces artificial patterns")

    biases.append("Original dataset class imbalance may favor majority purchase intention patterns")

    # 5. Geographic/Cultural bias
    print("\n\n5. CONTEXTUAL LIMITATIONS:")
    print("-" * 80)

    print("\nThese results are specific to:")
    print("  • The grocery store(s) in the original study")
    print("  • The geographic/cultural context of data collection")
    print("  • The time period when data was collected")
    print("\nGeneralization to other contexts should be done cautiously")

    biases.append("Results may not generalize to different store types, regions, or time periods")

    return biases

biases_list = analyze_biases(rankings_df, patterns_df)

# -----------------------------------------------------------------------------
# Step 5: Executive Summary
# -----------------------------------------------------------------------------
print("\n[Step 5] Creating Executive Summary")

def create_executive_summary(segments, recommendations_df, biases_list):
    """
    Create a concise executive summary for presentation.
    """
    print(f"\n{'='*80}")
    print("EXECUTIVE SUMMARY: REAL-WORLD IMPLICATIONS")
    print('='*80)

    print("\nKEY FINDINGS:")
    print("-" * 80)

    print(f"\n1. Customer Segmentation:")
    print(f"   • Identified {len(segments)} distinct purchase intention segments")
    print(f"   • Each segment requires different marketing approach")

    print(f"\n2. Demographic Drivers:")
    for target, segment_info in segments.items():
        print(f"\n   {target} ({segment_info['intention']}):")
        for demo, info in list(segment_info['profile'].items())[:2]:  # Top 2
            print(f"     - {demo}: {info['value']} ({info['prevalence']:.0f}%)")

    print(f"\n3. Marketing Priorities:")
    for _, rec in recommendations_df.iterrows():
        print(f"\n   {rec['Target']}: {rec['Objective']}")
        print(f"     Primary tactic: {rec['Tactics'][0]}")

    print(f"\n\nCRITICAL LIMITATIONS:")
    print("-" * 80)
    for i, bias in enumerate(biases_list[:5], 1):  # Top 5 biases
        print(f"{i}. {bias}")

    print(f"\n\nRECOMMENDATIONS FOR IMPLEMENTATION:")
    print("-" * 80)
    print("1. Start with A/B testing on small customer segments")
    print("2. Monitor actual conversion rates vs. predicted probabilities")
    print("3. Collect additional demographic data to reduce bias")
    print("4. Regularly retrain models with new purchase data")
    print("5. Consider external validation with different store locations")

executive_summary = create_executive_summary(segments, recommendations_df, biases_list)

"""### **DATASET CHALLENGES AND LIMITATIONS**

**1. SMALL SAMPLE SIZE:**

---
   - Limited number of observations restricts model reliability
   - Rare demographic combinations have insufficient data
   - Statistical power is reduced for minority classes
   - Confidence intervals are wider

**2. CLASS IMBALANCE:**

---
   - Unequal distribution of purchase intentions
   - Majority class dominates training
   - Minority classes are poorly represented
   - Synthetic balancing introduces artificial patterns

**3. MISSING DEMOGRAPHIC COVERAGE:**

---
   - Not all demographic combinations exist in data
   - Some customer segments are completely absent
   - Limits generalizability to broader population
   - Inference on missing combinations is speculative

**4. VARIABLE SELECTION IN BAYESIAN NETWORKS:**

---
   - Different models use different demographic variables
   - Makes direct comparison challenging
   - Some demographics may be more important but not included
   - Structure learning algorithms made different choices

**5. SYNTHETIC DATA QUALITY:**

---
   - Synthetic samples may not represent real customers
   - SMOTE interpolation can create impossible combinations
   - GAN-generated data may have mode collapse issues
   - Validation of synthetic data is difficult

**6. INTERPRETATION CHALLENGES:**

---
   - Probabilities from balanced models don't reflect real prevalence
   - Need domain expertise to validate patterns
   - Risk of overconfidence in model predictions
   - Business decisions require careful contextualization
"""

print("\nGenerating Final Summary Report")

summary_report = f"""
{'='*80}
ASSIGNMENT 3: DEMOGRAPHIC PROFILING - SUMMARY REPORT
{'='*80}

ANALYSIS OVERVIEW:
-----------------
• Total Models Analyzed: {len(all_models_info)}
• Total Demographic Combinations Tested: {sum(combo['count'] for combo in model_combinations.values()):,}
• Total Inferences Performed: {len(results_df):,}
• Top Profiles Identified per Intention: 20

KEY FINDINGS:
------------
1. Most Predictive Demographics:
{patterns_df[patterns_df['Percentage'] > 70]['Demographic'].value_counts().head().to_string()}

2. Model Consistency:
   Average Jaccard Similarity: {consistency_df['Jaccard_Similarity'].mean():.2%}

3. Probability Distribution Shifts:
   Mean absolute difference: {abs(prob_comp_df['Mean_Diff']).mean():.4f}
"""
print(summary_report)